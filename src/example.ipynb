{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd2eacd-c3ee-429f-9e53-03fdf253b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "                    .appName('helloSpark')\n",
    "                    .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3ec7cc-5d7b-4319-b704-f2fc139af563",
   "metadata": {},
   "source": [
    "Further info on Spark sessions:  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/spark_session.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f63dc4-1b2f-4959-9437-24fa9f592b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce07dab-127a-47b9-8b41-fe5ff62fb668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate some data for analysis\n",
    "\n",
    "import random\n",
    "\n",
    "names = [\"Alice\", \"Ben\", \"Charles\", \"Daisy\"]\n",
    "start_range = 900\n",
    "end_range = 5000\n",
    "python_data = [[random.choice(names),random.randint(start_range,end_range)] for i in range(500000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7fb01b-149a-416e-a09a-6ef50af25e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read in a Python object (list, dict), we can use spark.createDataFrame\n",
    "# We define a schema to have nicer column names and avoid Spark having to infer the schema\n",
    "schema = \"name STRING, salary INT\"\n",
    "\n",
    "df = spark.createDataFrame(python_data,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bc08ce-a374-499c-b98c-2e957dbebd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to display some rows, you can use .show() \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d767b21f-c3fe-4516-9360-8dfb091bcd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = (df.groupBy(\"name\")\n",
    "          .avg(\"salary\")\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c569ead7-8bb5-4a5e-8085-f7e6f690b3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc6c62f-d547-4f83-8fa6-5784aa35138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many of the functions hide behind spark.sql.functions\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "(df_new.select(\n",
    "    \"name\"\n",
    "    ,\"avg(salary)\"\n",
    "    ,F.round(\"avg(salary)\").alias(\"average\")\n",
    "    ).show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3474a14-484b-433e-a794-c5533d4b5087",
   "metadata": {},
   "source": [
    "The following is for a comparison with the popular Python package `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce745d0-a5d7-435a-9e40-1a3d01b8e59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "pd_df = pd.DataFrame(python_data,columns=[\"name\",\"salary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7590a6d-41c5-453d-8cdc-74e38f313634",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bde6d2-ebf6-4e88-ae66-7f8bf6e23883",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df.groupby(\"name\").mean(\"salary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0463103-efad-4c27-8698-bea7c575e7e9",
   "metadata": {},
   "source": [
    "Which of these seemed to be faster?  \n",
    "Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a323d65",
   "metadata": {},
   "source": [
    "Let's have a quick walkthrough of a few more PySpark methods.  \n",
    "For a longer (full) list of methods, see:  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048b9e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load CSV files into DataFrames\n",
    "employees_df = spark.read.option(\"header\", \"true\").csv(\"input/employees.csv\")\n",
    "departments_df = spark.read.option(\"header\", \"true\").csv(\"input/departments.csv\")\n",
    "\n",
    "employees_df.show()\n",
    "departments_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cffedd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert salary from string to integer for proper filtering\n",
    "employees_df = employees_df.withColumn(\"salary\", F.col(\"salary\").cast(\"integer\"))\n",
    "\n",
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ed39aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use .filter() for ... filtering.\n",
    "filtered_df = employees_df.filter(F.col(\"salary\") > 55000)\n",
    "\n",
    "# use .select() for ... selecting (columns).\n",
    "selected_df = filtered_df.select(\"emp_id\", \"name\", \"department\", \"salary\")\n",
    "\n",
    "# use withColumnRenamed for renaming columns\n",
    "renamed_df = selected_df.withColumnRenamed(\"emp_id\", \"employee_id\")\n",
    "\n",
    "# use selectExpr() for projecting SQL expressions\n",
    "expr_df = renamed_df.selectExpr(\"employee_id\", \"name\", \"department\", \"salary\", \"salary / 12 as monthly_salary\")\n",
    "\n",
    "# use .join() for ... joining. Let's join with departments DataFrame on the 'department' column\n",
    "joined_df = expr_df.join(departments_df, on=\"department\", how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36336f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# .write for writing. There are multiple more options you can see in the next classes.\n",
    "joined_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"output/joined_employees\")\n",
    "\n",
    "# NB - Spark has lazy evaluation. It will only execute the code when it needs to.\n",
    "# This means that you can chain multiple transformations and actions together without any performance hit.\n",
    "# The code will only be executed when you call an action like .show() or .write()."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
