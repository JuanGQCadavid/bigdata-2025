{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fd2eacd-c3ee-429f-9e53-03fdf253b9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 34222)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "                    .appName('helloSpark')\n",
    "                    .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3ec7cc-5d7b-4319-b704-f2fc139af563",
   "metadata": {},
   "source": [
    "Further info on Spark sessions:  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/spark_session.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01f63dc4-1b2f-4959-9437-24fa9f592b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b4d40971b65c:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>helloSpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff7fa19bd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ce07dab-127a-47b9-8b41-fe5ff62fb668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate some data for analysis\n",
    "\n",
    "import random\n",
    "\n",
    "names = [\"Alice\", \"Ben\", \"Charles\", \"Daisy\"]\n",
    "start_range = 900\n",
    "end_range = 5000\n",
    "python_data = [[random.choice(names),random.randint(start_range,end_range)] for i in range(500000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a7fb01b-149a-416e-a09a-6ef50af25e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read in a Python object (list, dict), we can use spark.createDataFrame\n",
    "# We define a schema to have nicer column names and avoid Spark having to infer the schema\n",
    "schema = \"name STRING, salary INT\"\n",
    "\n",
    "df = spark.createDataFrame(python_data,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44bc08ce-a374-499c-b98c-2e957dbebd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|salary|\n",
      "+-------+------+\n",
      "|Charles|  1292|\n",
      "|    Ben|  1217|\n",
      "|  Alice|  4010|\n",
      "|    Ben|  3203|\n",
      "|  Daisy|  2405|\n",
      "|  Alice|  4748|\n",
      "|    Ben|  3534|\n",
      "|  Alice|   995|\n",
      "|  Daisy|  2008|\n",
      "|  Alice|  2941|\n",
      "|Charles|  2699|\n",
      "|    Ben|  4154|\n",
      "|  Alice|  3443|\n",
      "|  Alice|  4015|\n",
      "|  Daisy|  3446|\n",
      "|    Ben|  3902|\n",
      "|Charles|  1691|\n",
      "|  Daisy|  4472|\n",
      "|    Ben|  3051|\n",
      "|  Alice|  3604|\n",
      "+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to display some rows, you can use .show() \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d767b21f-c3fe-4516-9360-8dfb091bcd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = (df.groupBy(\"name\")\n",
    "          .avg(\"salary\")\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c569ead7-8bb5-4a5e-8085-f7e6f690b3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|   name|       avg(salary)|\n",
      "+-------+------------------+\n",
      "|Charles| 2953.064667149693|\n",
      "|    Ben|2955.2915893382824|\n",
      "|  Alice|2944.0039734142465|\n",
      "|  Daisy| 2950.613626348975|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# It is lazy, so show actually perform all the work\n",
    "\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dc6c62f-d547-4f83-8fa6-5784aa35138b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------+\n",
      "|   name|       avg(salary)|average|\n",
      "+-------+------------------+-------+\n",
      "|Charles| 2953.064667149693| 2953.0|\n",
      "|    Ben|2955.2915893382824| 2955.0|\n",
      "|  Alice|2944.0039734142465| 2944.0|\n",
      "|  Daisy| 2950.613626348975| 2951.0|\n",
      "+-------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Many of the functions hide behind spark.sql.functions\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "(df_new.select(\n",
    "    \"name\"\n",
    "    ,\"avg(salary)\"\n",
    "    ,F.round(\"avg(salary)\").alias(\"average\")\n",
    "    ).show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3474a14-484b-433e-a794-c5533d4b5087",
   "metadata": {},
   "source": [
    "The following is for a comparison with the popular Python package `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ce745d0-a5d7-435a-9e40-1a3d01b8e59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "pd_df = pd.DataFrame(python_data,columns=[\"name\",\"salary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7590a6d-41c5-453d-8cdc-74e38f313634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Charles</td>\n",
       "      <td>1292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ben</td>\n",
       "      <td>1217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alice</td>\n",
       "      <td>4010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ben</td>\n",
       "      <td>3203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Daisy</td>\n",
       "      <td>2405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499995</th>\n",
       "      <td>Charles</td>\n",
       "      <td>2812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499996</th>\n",
       "      <td>Alice</td>\n",
       "      <td>2390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499997</th>\n",
       "      <td>Ben</td>\n",
       "      <td>3063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499998</th>\n",
       "      <td>Ben</td>\n",
       "      <td>3291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499999</th>\n",
       "      <td>Charles</td>\n",
       "      <td>3326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           name  salary\n",
       "0       Charles    1292\n",
       "1           Ben    1217\n",
       "2         Alice    4010\n",
       "3           Ben    3203\n",
       "4         Daisy    2405\n",
       "...         ...     ...\n",
       "499995  Charles    2812\n",
       "499996    Alice    2390\n",
       "499997      Ben    3063\n",
       "499998      Ben    3291\n",
       "499999  Charles    3326\n",
       "\n",
       "[500000 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19bde6d2-ebf6-4e88-ae66-7f8bf6e23883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alice</th>\n",
       "      <td>2944.003973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ben</th>\n",
       "      <td>2955.291589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Charles</th>\n",
       "      <td>2953.064667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Daisy</th>\n",
       "      <td>2950.613626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              salary\n",
       "name                \n",
       "Alice    2944.003973\n",
       "Ben      2955.291589\n",
       "Charles  2953.064667\n",
       "Daisy    2950.613626"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df.groupby(\"name\").mean(\"salary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0463103-efad-4c27-8698-bea7c575e7e9",
   "metadata": {},
   "source": [
    "Which of these seemed to be faster?  \n",
    "Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a323d65",
   "metadata": {},
   "source": [
    "Let's have a quick walkthrough of a few more PySpark methods.  \n",
    "For a longer (full) list of methods, see:  \n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048b9e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load CSV files into DataFrames\n",
    "employees_df = spark.read.option(\"header\", \"true\").csv(\"input/employees.csv\")\n",
    "departments_df = spark.read.option(\"header\", \"true\").csv(\"input/departments.csv\")\n",
    "\n",
    "employees_df.show()\n",
    "departments_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cffedd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert salary from string to integer for proper filtering\n",
    "employees_df = employees_df.withColumn(\"salary\", F.col(\"salary\").cast(\"integer\"))\n",
    "\n",
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ed39aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use .filter() for ... filtering.\n",
    "filtered_df = employees_df.filter(F.col(\"salary\") > 55000)\n",
    "\n",
    "# use .select() for ... selecting (columns).\n",
    "selected_df = filtered_df.select(\"emp_id\", \"name\", \"department\", \"salary\")\n",
    "\n",
    "# use withColumnRenamed for renaming columns\n",
    "renamed_df = selected_df.withColumnRenamed(\"emp_id\", \"employee_id\")\n",
    "\n",
    "# use selectExpr() for projecting SQL expressions\n",
    "expr_df = renamed_df.selectExpr(\"employee_id\", \"name\", \"department\", \"salary\", \"salary / 12 as monthly_salary\")\n",
    "\n",
    "# use .join() for ... joining. Let's join with departments DataFrame on the 'department' column\n",
    "joined_df = expr_df.join(departments_df, on=\"department\", how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36336f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# .write for writing. There are multiple more options you can see in the next classes.\n",
    "joined_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"output/joined_employees\")\n",
    "\n",
    "# NB - Spark has lazy evaluation. It will only execute the code when it needs to.\n",
    "# This means that you can chain multiple transformations and actions together without any performance hit.\n",
    "# The code will only be executed when you call an action like .show() or .write()."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
